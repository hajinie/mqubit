{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**seed 고정**\n",
    "\n",
    "seed 고정 이유:\n",
    "재현성을 보장하기 위해/ 동일한 데이터와 동일한 코드로 항상 동일한 결과를 얻음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11577/4053568624.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train['Fingerprint'] = train['Smiles'].apply(smiles_to_fingerprint)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1779.585927547353\n",
      "MAE: 514.1858517109845\n",
      "R² Score: 0.16630680210327797\n"
     ]
    }
   ],
   "source": [
    "CFG = {\n",
    "    'NBITS':2048,\n",
    "    'SEED':42,\n",
    "    'N_ESTIMATORS' :200,\n",
    "\n",
    "}\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    return f\"Seed {seed} has been set.\"\n",
    "seed_everything(CFG['SEED'])\n",
    "\n",
    "# SMILES 데이터를 분자 지문으로 변환\n",
    "def smiles_to_fingerprint(Smiles):\n",
    "    mol = Chem.MolFromSmiles(Smiles)\n",
    "    if mol is not None:\n",
    "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=CFG['NBITS'])\n",
    "        return np.array(fp)\n",
    "    else:\n",
    "        return np.zeros((CFG['NBITS'],))\n",
    "    \n",
    "\n",
    "\n",
    "# 학습 ChEMBL 데이터 로드\n",
    "chembl_data = pd.read_csv('./open/train.csv')  # 예시 파일 이름\n",
    "chembl_data.head()\n",
    "\n",
    "train = chembl_data[['Smiles', 'pIC50']]\n",
    "train['Fingerprint'] = train['Smiles'].apply(smiles_to_fingerprint)\n",
    "train_x = np.stack(train['Fingerprint'].values)\n",
    "train_y = train['pIC50'].values\n",
    "\n",
    "\n",
    "# 학습 및 검증 데이터 분리\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.25, random_state=42)\n",
    "\n",
    "# 랜덤 포레스트 모델 학습\n",
    "model = RandomForestRegressor(max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100, random_state=42)\n",
    "model.fit(train_x, train_y)\n",
    "\n",
    "\n",
    "def pIC50_to_IC50(pic50_values):\n",
    "    \"\"\"Convert pIC50 values to IC50 (nM).\"\"\"\n",
    "    return 10 ** (9 - pic50_values)\n",
    "\n",
    "# Validation 데이터로부터의 학습 모델 평가\n",
    "val_y_pred = model.predict(val_x)\n",
    "\n",
    "# IC50 단위로 변환된 값\n",
    "val_y_ic50 = pIC50_to_IC50(val_y)\n",
    "val_y_pred_ic50 = pIC50_to_IC50(val_y_pred)\n",
    "\n",
    "mse = mean_squared_error(val_y_ic50, val_y_pred_ic50)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(val_y_ic50, val_y_pred_ic50)\n",
    "r2 = r2_score(val_y_ic50, val_y_pred_ic50)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R² Score: {r2}')\n",
    "\n",
    "#파일 저장하기\n",
    "\n",
    "test = pd.read_csv('./open/test.csv')\n",
    "test['Fingerprint'] = test['Smiles'].apply(smiles_to_fingerprint)\n",
    "\n",
    "test_x = np.stack(test['Fingerprint'].values)\n",
    "test_y_pred = model.predict(test_x)\n",
    "\n",
    "submit = pd.read_csv('./open/sample_submission.csv')\n",
    "submit['IC50_nM'] = pIC50_to_IC50(test_y_pred)\n",
    "submit.head()\n",
    "\n",
    "submit.to_csv('./open/submit_file.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "from rdkit.Chem import AllChem\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.9547830068802042\n",
      "[6.53590417 7.56868333 7.844055   7.6322     7.5886     7.90321333\n",
      " 7.615205   7.63235    7.54309333 6.84411548 6.84471548 7.73174333\n",
      " 7.5914     6.54315    7.5478     7.48505    7.60533333 7.59830333\n",
      " 7.79888833 7.75123333 7.838155   7.5958     7.147175   7.94406333\n",
      " 7.837005   7.64125    7.76336333 7.63196667 7.84062833 7.803655\n",
      " 7.88973833 7.70244833 7.683805   7.21837833 7.25275833 7.79508833\n",
      " 7.32732167 7.34355    7.73183    7.537425   7.849205   7.72040833\n",
      " 7.73645833 7.838155   7.520825   8.26883    7.06931548 6.98056548\n",
      " 7.019425   7.77123833 7.397175   7.79450833 7.4046     7.86461333\n",
      " 7.5941     7.81027833 7.0486625  7.10465    7.63161667 7.52401667\n",
      " 7.0332375  7.922855   7.81677833 7.60243333 7.60291667 7.71039833\n",
      " 7.5344     7.55695    7.65328833 7.66253333 7.792505   7.05545\n",
      " 7.90108333 7.24035    7.71048833 7.43390333 7.645      7.81005833\n",
      " 7.825505   7.66056667 7.6928     7.620375   7.63886667 7.92666333\n",
      " 7.4253     7.50918333 7.64238833 7.57631667 7.093725   7.73386667\n",
      " 7.49543333 7.37978333 7.80168    7.45956    7.58673    7.0470125\n",
      " 7.60291667 7.49439333 7.227225   7.680825   7.88973833 7.59335\n",
      " 7.14835    7.641825   7.53948333 7.0248875  7.62745    7.41582833\n",
      " 7.665      7.0262875  7.838155   7.67490833 7.53395   ]\n",
      "RMSE: 1779.585927547353\n",
      "MAE: 514.1858517109845\n",
      "R² Score: 0.16630680210327797\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 데이터 로드\n",
    "chembl_data = pd.read_csv('./open/train.csv')\n",
    "\n",
    "# SMILES와 pIC50 추출\n",
    "chembl_data = chembl_data[['Smiles', 'pIC50']]\n",
    "\n",
    "# DeepChem의 featurizer를 이용해 SMILES 데이터를 피처로 변환\n",
    "featurizer = dc.feat.CircularFingerprint(size=2048)  # Morgan Fingerprint 사용\n",
    "features = featurizer.featurize(chembl_data['Smiles'])\n",
    "\n",
    "# DeepChem의 Dataset 형식으로 변환\n",
    "dataset = dc.data.NumpyDataset(X=features, y=chembl_data['pIC50'].values)\n",
    "\n",
    "\n",
    "# RandomForestRegressor를 DeepChem의 SklearnModel 래퍼로 감싸기\n",
    "rf_model = dc.models.SklearnModel(RandomForestRegressor(n_estimators=200))\n",
    "\n",
    "# 모델 학습\n",
    "rf_model.fit(dataset)\n",
    "\n",
    "# 모델 평가\n",
    "metric = dc.metrics.Metric(dc.metrics.r2_score)\n",
    "score = rf_model.evaluate(dataset, [metric])\n",
    "# print(f\"R^2 Score: {score['r2_score']}\")\n",
    "\n",
    "# Validation set 평가\n",
    "val_data = pd.read_csv('./open/test.csv')\n",
    "val_features = featurizer.featurize(val_data['Smiles'])\n",
    "val_dataset = dc.data.NumpyDataset(X=val_features)\n",
    "\n",
    "# 예측 수행\n",
    "predictions = rf_model.predict(val_dataset)\n",
    "print(predictions)\n",
    "\n",
    "# IC50 값으로 변환 및 저장\n",
    "def pIC50_to_IC50(pic50_values):\n",
    "    return 10 ** (9 - pic50_values)\n",
    "\n",
    "val_y_ic50 = pIC50_to_IC50(val_y)\n",
    "val_y_pred_ic50 = pIC50_to_IC50(val_y_pred)\n",
    "\n",
    "mse = mean_squared_error(val_y_ic50, val_y_pred_ic50)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(val_y_ic50, val_y_pred_ic50)\n",
    "r2 = r2_score(val_y_ic50, val_y_pred_ic50)\n",
    "\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'R² Score: {r2}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "submit = pd.DataFrame({'SMILES': val_data['Smiles'], 'IC50_nM': predicted_ic50})\n",
    "submit.to_csv('./open/submit_file.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method MessagePassing.call of <deepchem.models.layers.MessagePassing object at 0x7f3d230552b0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method MessagePassing.call of <deepchem.models.layers.MessagePassing object at 0x7f3d230552b0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method EdgeNetwork.call of <deepchem.models.layers.EdgeNetwork object at 0x7f3d23030130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method EdgeNetwork.call of <deepchem.models.layers.EdgeNetwork object at 0x7f3d23030130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method GatedRecurrentUnit.call of <deepchem.models.layers.GatedRecurrentUnit object at 0x7f3d23030310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method GatedRecurrentUnit.call of <deepchem.models.layers.GatedRecurrentUnit object at 0x7f3d23030310>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:34:08.671292: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2024-08-21 15:34:08.673768: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 15:34:08.687020: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <bound method SetGather.call of <deepchem.models.layers.SetGather object at 0x7f3d22e26f70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method SetGather.call of <deepchem.models.layers.SetGather object at 0x7f3d22e26f70>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method TrimGraphOutput.call of <deepchem.models.graph_models.TrimGraphOutput object at 0x7f3d23a9cbe0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method TrimGraphOutput.call of <deepchem.models.graph_models.TrimGraphOutput object at 0x7f3d23a9cbe0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.keras.optimizers' has no attribute 'legacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 45\u001b[0m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m MPNNModel(\n\u001b[1;32m     30\u001b[0m     n_tasks\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,  \u001b[38;5;66;03m# Single regression task\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mregression\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39mseed\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Validate the model\u001b[39;00m\n\u001b[1;32m     48\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(val_dataset)\n",
      "File \u001b[0;32m~/miniforge3/envs/dacon/lib/python3.8/site-packages/deepchem/models/keras_model.py:359\u001b[0m, in \u001b[0;36mKerasModel.fit\u001b[0;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    310\u001b[0m         dataset: Dataset,\n\u001b[1;32m    311\u001b[0m         nb_epoch: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    318\u001b[0m         callbacks: Union[Callable, List[Callable]] \u001b[38;5;241m=\u001b[39m [],\n\u001b[1;32m    319\u001b[0m         all_losses: Optional[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m    320\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train this model on a dataset.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \n\u001b[1;32m    322\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        The average loss over the most recent checkpoint interval\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_generator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnb_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_checkpoints_to_keep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_losses\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/dacon/lib/python3.8/site-packages/deepchem/models/keras_model.py:412\u001b[0m, in \u001b[0;36mKerasModel.fit_generator\u001b[0;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, SequenceCollection):\n\u001b[1;32m    411\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [callbacks]\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_built\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_interval \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    414\u001b[0m     manager \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mCheckpointManager(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint,\n\u001b[1;32m    415\u001b[0m                                          \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dir,\n\u001b[1;32m    416\u001b[0m                                          max_checkpoints_to_keep)\n",
      "File \u001b[0;32m~/miniforge3/envs/dacon/lib/python3.8/site-packages/deepchem/models/keras_model.py:270\u001b[0m, in \u001b[0;36mKerasModel._ensure_built\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_built \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_global_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mVariable(\u001b[38;5;241m0\u001b[39m, trainable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 270\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_tf_optimizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_global_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mtrain\u001b[38;5;241m.\u001b[39mCheckpoint(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tf_optimizer,\n\u001b[1;32m    273\u001b[0m                                        model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n",
      "File \u001b[0;32m~/miniforge3/envs/dacon/lib/python3.8/site-packages/deepchem/models/optimizers.py:222\u001b[0m, in \u001b[0;36mAdam._create_tf_optimizer\u001b[0;34m(self, global_step)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    221\u001b[0m     learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearning_rate\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlegacy\u001b[49m\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate,\n\u001b[1;32m    223\u001b[0m                                        beta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta1,\n\u001b[1;32m    224\u001b[0m                                        beta_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta2,\n\u001b[1;32m    225\u001b[0m                                        epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.keras.optimizers' has no attribute 'legacy'"
     ]
    }
   ],
   "source": [
    "import deepchem as dc\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from deepchem.models import MPNNModel\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Load the dataset\n",
    "data_path = './open/train.csv'\n",
    "chembl_data = pd.read_csv(data_path)\n",
    "\n",
    "# Prepare features and targets\n",
    "featurizer = dc.feat.MolGraphConvFeaturizer()\n",
    "X = featurizer.featurize(chembl_data['Smiles'])\n",
    "y = chembl_data['pIC50'].values\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Create DeepChem NumpyDatasets for training and validation\n",
    "train_dataset = dc.data.NumpyDataset(X_train, y_train)\n",
    "val_dataset = dc.data.NumpyDataset(X_val, y_val)\n",
    "\n",
    "# Define the MPNN Model\n",
    "model = MPNNModel(\n",
    "    n_tasks=1,  # Single regression task\n",
    "    mode='regression', \n",
    "    number_of_molecules=1, \n",
    "    n_atom_feat=75, \n",
    "    n_pair_feat=14, \n",
    "    n_hidden=128, \n",
    "    T=5, \n",
    "    M=3, \n",
    "    batch_size=32, \n",
    "    learning_rate=1e-3,\n",
    "    use_queue=True,\n",
    "    random_seed=seed\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, nb_epoch=100)\n",
    "\n",
    "# Validate the model\n",
    "y_pred = model.predict(val_dataset)\n",
    "\n",
    "# Evaluate model performance\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "\n",
    "print(f'Validation RMSE: {rmse}')\n",
    "print(f'Validation R²: {r2}')\n",
    "\n",
    "# Load the test set and predict IC50 values\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "X_test = featurizer.featurize(test_data['Smiles'])\n",
    "test_dataset = dc.data.NumpyDataset(X_test)\n",
    "test_pred = model.predict(test_dataset)\n",
    "\n",
    "# Save the predictions\n",
    "submit = pd.DataFrame({'SMILES': test_data['Smiles'], 'Predicted_IC50': test_pred.flatten()})\n",
    "submit.to_csv('./submission.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teachopencadd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
